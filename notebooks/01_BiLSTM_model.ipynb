{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop/.cache/pypoetry/virtualenvs/ner-analysis-9AuOIkqF-py3.8/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    TimeDistributed,\n",
    "    Dense,\n",
    "    Layer,\n",
    ")\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data/raw/gmb-2.2.0/data/\"\n",
    "\n",
    "fnames = []\n",
    "for root, dirs, files in os.walk(data_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            fnames.append(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/gmb-2.2.0/data/p37/d0625/en.tags',\n",
       " '../data/raw/gmb-2.2.0/data/p37/d0686/en.tags']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = collections.Counter()\n",
    "iob_tags = collections.Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ner_subcat(tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip the NER subcategory from a tag.\n",
    "    \"\"\"\n",
    "    return tag.split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob_format(ners: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts IO tags into IOB format\n",
    "    \"\"\"\n",
    "    iob_tokens = []\n",
    "    for idx, token in enumerate(ners):\n",
    "        if token != \"O\":\n",
    "            if idx == 0:\n",
    "                token = \"B-\" + token\n",
    "            elif ners[idx - 1] == token:\n",
    "                token = \"I-\" + token\n",
    "            else:\n",
    "                token = \"B-\" + token\n",
    "\n",
    "        iob_tokens.append(token)\n",
    "        iob_tags[token] += 1\n",
    "\n",
    "    return iob_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A counter is set for the **number of sentences**. \n",
    "2. A list of files written with paths are also initialized. \n",
    "3. As processed files are written out, their paths are added to the\n",
    "outfiles variable. \n",
    "4. This list will be used later to load all the data and to train the\n",
    "model. \n",
    "5. Files are read and split into two empty newline characters. That is the marker\n",
    "for the end of a sentence in the file. Only the actual words, **POS** tokens, and **NER**\n",
    "tokens are used from the file. \n",
    "6. Once these are collected, a new CSV file is written with\n",
    "three columns: the sentence, a sequence of **POS** tags, and a sequence of **NER** tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = 0\n",
    "outfiles = []\n",
    "path = \"../data/preprocessed/ner/\"\n",
    "for idx, file in enumerate(fnames):\n",
    "    with open(file, \"rb\") as content:\n",
    "        data = content.read().decode(\"utf-8\").strip()\n",
    "        sentences = data.split(\"\\n\\n\")\n",
    "        print(idx, file, len(sentences))\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        with open(path + str(idx) + \"-\" + os.path.basename(file), \"w\") as outfile:\n",
    "            outfiles.append(path + str(idx) + \"-\" + os.path.basename(file))\n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                toks = sentence.split(\"\\n\")\n",
    "                words, pos, ner = [], [], []\n",
    "\n",
    "                for tok in toks:\n",
    "                    t = tok.split(\"\\t\")\n",
    "                    words.append(t[0])\n",
    "                    pos.append(t[1])\n",
    "                    ner_tags[t[3]] += 1\n",
    "                    ner.append(strip_ner_subcat(t[3]))\n",
    "                writer.writerow(\n",
    "                    [\" \".join(words), \" \".join(iob_format(ner)), \" \".join(pos)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences:  62010\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of sentences: \", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 1146068, 'geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'org-leg': 60, 'per-ini': 60, 'per-ord': 38, 'tim-dom': 10, 'per-mid': 1, 'art-add': 1})\n",
      "Counter({'O': 1146068, 'B-geo': 48876, 'B-tim': 26296, 'B-org': 26195, 'I-per': 22270, 'B-per': 21984, 'I-org': 21899, 'B-gpe': 20436, 'I-geo': 9512, 'I-tim': 8493, 'B-art': 503, 'B-eve': 391, 'I-art': 364, 'I-eve': 318, 'I-gpe': 244, 'B-nat': 238, 'I-nat': 62})\n"
     ]
    }
   ],
   "source": [
    "print(ner_tags)\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing and vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/preprocessed/ner/*.tags\")\n",
    "\n",
    "data_pd = pd.concat(\n",
    "    [pd.read_csv(f, header=None, names=[\"text\", \"label\", \"pos\"]) for f in files],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62010 entries, 0 to 62009\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    62010 non-null  object\n",
      " 1   label   62010 non-null  object\n",
      " 2   pos     62010 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tok = Tokenizer(filters=\"[\\\\]^\\t\\n\", lower=False, split=\" \", oov_token=\"<OOV>\")\n",
    "\n",
    "pos_tok = Tokenizer(filters=\"\\t\\n\", lower=False, split=\" \", oov_token=\"<OOV>\")\n",
    "\n",
    "ner_tok = Tokenizer(filters=\"\\t\\n\", lower=False, split=\" \", oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tok.fit_on_texts(data_pd[\"text\"])\n",
    "pos_tok.fit_on_texts(data_pd[\"pos\"])\n",
    "ner_tok.fit_on_texts(data_pd[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_config = ner_tok.get_config()\n",
    "text_config = text_tok.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_words': None, 'filters': '\\t\\n', 'lower': False, 'split': ' ', 'char_level': False, 'oov_token': '<OOV>', 'document_count': 62010, 'word_counts': '{\"B-gpe\": 20436, \"O\": 1146068, \"B-geo\": 48876, \"B-tim\": 26296, \"B-per\": 21984, \"I-per\": 22270, \"B-org\": 26195, \"I-org\": 21899, \"I-geo\": 9512, \"B-nat\": 238, \"I-nat\": 62, \"I-tim\": 8493, \"B-art\": 503, \"B-eve\": 391, \"I-eve\": 318, \"I-gpe\": 244, \"I-art\": 364}', 'word_docs': '{\"O\": 61999, \"B-gpe\": 16565, \"B-geo\": 31660, \"B-tim\": 22345, \"B-per\": 17499, \"I-per\": 13805, \"B-org\": 20478, \"I-org\": 11011, \"I-geo\": 7738, \"B-nat\": 211, \"I-nat\": 50, \"I-tim\": 5526, \"B-art\": 425, \"I-eve\": 201, \"B-eve\": 361, \"I-gpe\": 224, \"I-art\": 207}', 'index_docs': '{\"2\": 61999, \"9\": 16565, \"3\": 31660, \"4\": 22345, \"7\": 17499, \"6\": 13805, \"5\": 20478, \"8\": 11011, \"10\": 7738, \"17\": 211, \"18\": 50, \"11\": 5526, \"12\": 425, \"15\": 201, \"13\": 361, \"16\": 224, \"14\": 207}', 'index_word': '{\"1\": \"<OOV>\", \"2\": \"O\", \"3\": \"B-geo\", \"4\": \"B-tim\", \"5\": \"B-org\", \"6\": \"I-per\", \"7\": \"B-per\", \"8\": \"I-org\", \"9\": \"B-gpe\", \"10\": \"I-geo\", \"11\": \"I-tim\", \"12\": \"B-art\", \"13\": \"B-eve\", \"14\": \"I-art\", \"15\": \"I-eve\", \"16\": \"I-gpe\", \"17\": \"B-nat\", \"18\": \"I-nat\"}', 'word_index': '{\"<OOV>\": 1, \"O\": 2, \"B-geo\": 3, \"B-tim\": 4, \"B-org\": 5, \"I-per\": 6, \"B-per\": 7, \"I-org\": 8, \"B-gpe\": 9, \"I-geo\": 10, \"I-tim\": 11, \"B-art\": 12, \"B-eve\": 13, \"I-art\": 14, \"I-eve\": 15, \"I-gpe\": 16, \"B-nat\": 17, \"I-nat\": 18}'}\n"
     ]
    }
   ],
   "source": [
    "print(ner_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = eval(text_config[\"index_word\"])\n",
    "ner_vocab = eval(ner_config[\"index_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in vocab: 39422\n",
      "Unique NER tags in vocab: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words in vocab:\", len(text_vocab))\n",
    "print(\"Unique NER tags in vocab:\", len(ner_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tok = text_tok.texts_to_sequences(data_pd[\"text\"])\n",
    "y_tok = ner_tok.texts_to_sequences(data_pd[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "\n",
    "x_pad = sequence.pad_sequences(x_tok, padding=\"post\", maxlen=max_len)\n",
    "\n",
    "y_pad = sequence.pad_sequences(y_tok, padding=\"post\", maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62010, 50), (62010, 50))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pad.shape, y_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62010, 50, 19)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(ner_vocab) + 1\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and traning the BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_vocab) + 1\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "rnn_units = 100\n",
    "\n",
    "BATCH_SIZE = 90\n",
    "\n",
    "num_classes = len(ner_vocab) + 1\n",
    "\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm(\n",
    "    vocab_size, embedding_dim, rnn_units, batch_size, classes\n",
    ") -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_dim,\n",
    "                mask_zero=True,\n",
    "                batch_input_shape=[batch_size, None],\n",
    "            ),\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    units=rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    dropout=dropout,\n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                )\n",
    "            ),\n",
    "            TimeDistributed(Dense(rnn_units, activation=\"relu\")),\n",
    "            Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:56:52.219885: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-23 16:56:52.235463: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-23 16:56:52.235492: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-23 16:56:52.235513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2023-10-23 16:56:52.238473: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "model = build_model_bilstm(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (90, None, 64)            2523072   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (90, None, 200)           132000    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (90, None, 100)           20100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (90, None, 19)            1919      \n",
      "=================================================================\n",
      "Total params: 2,677,091\n",
      "Trainable params: 2,677,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_bilstm(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=num_classes,\n",
    ")\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_pad\n",
    "\n",
    "# create training and testing splits\n",
    "total_sentences = 62010\n",
    "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
    "X_train = X[BATCH_SIZE * test_size :]\n",
    "Y_train = Y[BATCH_SIZE * test_size :]\n",
    "\n",
    "X_test = X[0 : BATCH_SIZE * test_size]\n",
    "Y_test = Y[0 : BATCH_SIZE * test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49590, 50), (49590, 50, 19), (12420, 50), (12420, 50, 19))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:56:55.723321: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188442000 exceeds 10% of free system memory.\n",
      "2023-10-23 16:56:56.008152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-10-23 16:56:56.048415: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3393245000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "551/551 [==============================] - 66s 110ms/step - loss: 0.3462 - accuracy: 0.8600\n",
      "Epoch 2/15\n",
      "551/551 [==============================] - 58s 106ms/step - loss: 0.0483 - accuracy: 0.9673\n",
      "Epoch 3/15\n",
      "551/551 [==============================] - 57s 104ms/step - loss: 0.0334 - accuracy: 0.9759\n",
      "Epoch 4/15\n",
      "551/551 [==============================] - 57s 104ms/step - loss: 0.0281 - accuracy: 0.9794\n",
      "Epoch 5/15\n",
      "551/551 [==============================] - 54s 99ms/step - loss: 0.0237 - accuracy: 0.9825\n",
      "Epoch 6/15\n",
      "551/551 [==============================] - 52s 94ms/step - loss: 0.0206 - accuracy: 0.9846\n",
      "Epoch 7/15\n",
      "551/551 [==============================] - 52s 95ms/step - loss: 0.0179 - accuracy: 0.9867\n",
      "Epoch 8/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0156 - accuracy: 0.9881\n",
      "Epoch 9/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0134 - accuracy: 0.9899\n",
      "Epoch 10/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0118 - accuracy: 0.9913\n",
      "Epoch 11/15\n",
      "551/551 [==============================] - 39s 72ms/step - loss: 0.0103 - accuracy: 0.9923\n",
      "Epoch 12/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0095 - accuracy: 0.9927\n",
      "Epoch 13/15\n",
      "551/551 [==============================] - 39s 72ms/step - loss: 0.0081 - accuracy: 0.9939\n",
      "Epoch 14/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0072 - accuracy: 0.9946\n",
      "Epoch 15/15\n",
      "551/551 [==============================] - 40s 72ms/step - loss: 0.0063 - accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc147ceae20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 5s 26ms/step - loss: 0.0943 - accuracy: 0.9621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09432904422283173, 0.9620593190193176]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/BiLSTM.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-analysis-9AuOIkqF-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
